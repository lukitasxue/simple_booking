import { CrawlConfig, CrawlProcessingResult, CrawlOutput } from '@/lib/general-config/general-config';
import { 
  initializeContentProcessing, 
  processAndEmbedChunksPerPage, 
  finalizeContentProcessing 
} from './process-content/grouping-storing-content';
import { logger as globalLoggerInstance } from './process-content/logger';
import { initializeCrawlerEnvironment } from './crawler-setup';
import { processPdfBuffersConcurrently, ProcessPdfsResult } from './pdf-fetcher/pdf-document-processor';
import { generateLlmSegmentedChunksForSinglePage } from './process-content/page-content-processor';
import type { LLMSegmentedChunk } from './process-content/LLMSegmenterCategorizer';
import { v4 as uuidv4 } from 'uuid';

/**
 * Initial processing of PDF buffers: extracts content from each PDF and prepares basic crawl results.
 * This function does not perform LLM-based segmentation or categorization.
 *
 * @param config The crawl configuration, primarily used for business ID and PDF names.
 * @param pdfBuffers An array of Buffers, each representing a PDF file's content.
 * @returns A Promise resolving to an object that extends CrawlOutput, including successfully processed base PDF identifiers.
 */
export async function crawlPdfs(
  config: CrawlConfig, 
  pdfBuffers: Buffer[]
): Promise<CrawlOutput & { successfullyProcessedBasePdfs: string[] }> {
  // Process all provided PDF buffers concurrently to extract text and metadata.
  const { 
    crawlResults, // Detailed results for each processed PDF (or pages within, depending on pdf-document-processor)
    successfullyProcessedBasePdfs, // Identifiers (e.g., filenames) of PDFs that were successfully processed at a basic level.
    allDiscoveredPageUrls // All URLs generated by pdf-document-processor (e.g., pdf:filename.pdf#page=1)
  }: ProcessPdfsResult = await processPdfBuffersConcurrently(config, pdfBuffers);

  // Log all unique page-specific URLs that were generated during PDF processing for tracking.
  await globalLoggerInstance.recordDiscoveredUrls(allDiscoveredPageUrls);

  // Filter out successfully processed PDF page URLs for the main output's URL list.
  const processedPageUrlsForOutput = crawlResults
    .filter(r => r.status === 'processed' && r.fullUrl.startsWith('pdf:'))
    .map(r => r.fullUrl);

  // Return structured output including crawl results, a list of processed PDF page URLs,
  // a placeholder for main language (as language is per-PDF), and successfully processed base PDF identifiers.
  return {
    results: crawlResults, 
    urls: processedPageUrlsForOutput, 
    mainLanguage: 'unknown', // PDF language is typically detected per document/page, not globally for the batch.
    successfullyProcessedBasePdfs
  };
}

/**
 * Determines a representative source name for PDF processing, typically the first PDF name provided.
 * @param config The crawl configuration, which might contain an array of PDF names.
 * @param successfullyProcessedBasePdfs A list of base PDF identifiers that were successfully processed.
 * @returns A string representing the source, or 'unknown.pdf' if no specific name can be determined.
 */
function _determinePdfSource(
  config: CrawlConfig, 
  successfullyProcessedBasePdfs: string[]
): string {
  if (config.pdfNames && config.pdfNames.length > 0) {
    return config.pdfNames[0]; // Prefer the first name from the config if available.
  }
  if (successfullyProcessedBasePdfs.length > 0) {
    return successfullyProcessedBasePdfs[0]; // Fallback to the first successfully processed PDF identifier.
  }
  return 'unknown.pdf'; // Default if no other source name can be found.
}

/**
 * Processes the content extracted from PDFs: generates LLM-friendly text chunks, categorizes them,
 * and prepares them for embedding or other downstream tasks.
 *
 * @param config The crawl configuration.
 * @param crawlOutput The output from the `crawlPdfs` function, containing extracted text and metadata.
 * @returns A Promise resolving to the processed content result, including merged text and embedding status.
 */
export async function processPdfContent(
    config: CrawlConfig, 
    crawlOutput: CrawlOutput & { successfullyProcessedBasePdfs: string[] }
): Promise<CrawlProcessingResult & { embeddingsStatus?: string }> {
  const { results, successfullyProcessedBasePdfs } = crawlOutput;
  const llmChunkGenSessionId = uuidv4();
  const domain = _determinePdfSource(config, successfullyProcessedBasePdfs);

  // Filter for PDF pages that were successfully processed and contain actual text content.
  const processablePdfPages = results.filter(r => 
    r.status === 'processed' && 
    r.cleanedText && 
    r.cleanedText.trim().length > 0 && // Ensure text is not just whitespace.
    r.fullUrl.startsWith('pdf:')
  );

  const determinedSource = _determinePdfSource(config, successfullyProcessedBasePdfs);

  // If no processable PDF pages are found, log and return an empty result.
  if (processablePdfPages.length === 0) {
    console.log('[PDF Crawler] No processed PDF pages with actual content to segment and categorize.');
    if(globalLoggerInstance.isInitialized) await globalLoggerInstance.logSummary();
    return {
        mergedText: '', pageCount: 0, uniqueParagraphs: 0,
        businessId: config.businessId,
        source: determinedSource,
        embeddingsStatus: 'No PDF content to process'
    };
  }

  // Initialize content processing (DB session, processing state)
  const processedPageUrlsForSession = processablePdfPages.map(r => r.fullUrl); 
  const successfullyProcessedRootUrlsForSession = successfullyProcessedBasePdfs;

  const { 
    sessionId: dbSessionId, 
    processingState, 
    sessionInstance 
  } = await initializeContentProcessing(config, processedPageUrlsForSession, successfullyProcessedRootUrlsForSession);
  
  console.log(`[PDF Crawler] DB Session initialized: ${dbSessionId}. ProcessingState created.`);

  const allPdfLlmSegmentedChunks: LLMSegmentedChunk[] = [];

  for (const pageResult of processablePdfPages) {
    if (!pageResult.cleanedText || !pageResult.fullUrl) {
      console.warn(
        `[PDF Crawler] Skipping page ${pageResult.fullUrl} for LLM chunking due to missing cleanedText or fullUrl.`
      );
      continue;
    }
    try {
      console.log(`[PDF Crawler] Generating LLM chunks for page: ${pageResult.fullUrl} (LLM Chunk Gen SID: ${llmChunkGenSessionId})`);
      const chunksFromPage = await generateLlmSegmentedChunksForSinglePage({
        pageResult,
        config,
        domain,
        sessionId: llmChunkGenSessionId,
      });
      
      if (chunksFromPage.length > 0) {
        allPdfLlmSegmentedChunks.push(...chunksFromPage);
        console.log(`[PDF Crawler] Generated ${chunksFromPage.length} LLM chunks for ${pageResult.fullUrl}.`);
        await processAndEmbedChunksPerPage(chunksFromPage, config, dbSessionId, processingState);
        console.log(`[PDF Crawler] Submitted ${chunksFromPage.length} chunks from ${pageResult.fullUrl} for embedding and DB processing (DB SID: ${dbSessionId}).`);
      } else {
        console.log(`[PDF Crawler] No LLM chunks generated for ${pageResult.fullUrl}.`);
      }
    } catch (error) {
      console.error(`[PDF Crawler] Error processing page ${pageResult.fullUrl} for LLM chunks or embedding:`, error);
    }
  }

  // If LLM segmentation yields no chunks, log a warning and return appropriate results.
  if (allPdfLlmSegmentedChunks.length === 0) {
    console.warn('[PDF Crawler] No PDF content was successfully segmented by LLM.');
    if (dbSessionId) {
        await finalizeContentProcessing(processingState, sessionInstance);
    }
    if(globalLoggerInstance.isInitialized) await globalLoggerInstance.logSummary();
    return {
        mergedText: '', 
        pageCount: processablePdfPages.length,
        uniqueParagraphs: 0,
        businessId: config.businessId,
        source: determinedSource,
        embeddingsStatus: 'No LLM-segmented chunks from PDF to process'
    };
  }

  // All page-by-page processing is done. Now finalize the entire batch.
  const finalStatus = await finalizeContentProcessing(processingState, sessionInstance);
  console.log(`[PDF Crawler] Content processing finalized. Final status/session ID: ${finalStatus}`);
  
  // Log a summary of the overall PDF processing task.
  await globalLoggerInstance.logSummary();

  // Concatenate all chunk texts to form a single merged text output for the PDFs.
  const allPdfTextFromLlmChunks = allPdfLlmSegmentedChunks.map((item: LLMSegmentedChunk) => item.chunkText).join('\n\n');

  // Return the final processing results.
  return {
    mergedText: allPdfTextFromLlmChunks,
    pageCount: processablePdfPages.length,
    uniqueParagraphs: allPdfLlmSegmentedChunks.length,
    businessId: config.businessId,
    source: determinedSource, 
    embeddingsStatus: `Processed with DB Session ID: ${dbSessionId}`,
  };
}

/**
 * Orchestrates the entire PDF crawling and content processing workflow.
 * It first processes PDF buffers to extract content, then processes this content for LLM segmentation and storage.
 *
 * @param config The crawl configuration.
 * @param pdfBuffers An array of Buffers, each representing a PDF file's content.
 * @returns A Promise resolving to the final processed content result from PDFs, including embedding status.
 */
export async function crawlAndProcessPdfs(
  config: CrawlConfig, 
  pdfBuffers: Buffer[]
): Promise<CrawlProcessingResult & { embeddingsStatus?: string }> {
  // Ensure the crawler environment (e.g., logger, artifact savers) is initialized.
  await initializeCrawlerEnvironment(); 

  // Step 1: Perform initial PDF processing to extract text and metadata from buffers.
  const crawlOutput = await crawlPdfs(config, pdfBuffers);
  
  // Check if any PDF pages were successfully processed at the basic level.
  if (crawlOutput.results.filter(r => r.status === 'processed').length === 0) {
    console.warn('[PDF Crawler] No PDF pages were successfully processed during initial crawl. Skipping content processing stage.');
    if(globalLoggerInstance.isInitialized) await globalLoggerInstance.logSummary(); // Log summary even if aborting early.
    // Determine a source name for reporting, even in case of early exit.
    const sourceForEarlyExit = (config.pdfNames && config.pdfNames.length > 0) ? config.pdfNames[0] : 'processed_pdfs';
    return {
        mergedText: '', pageCount: 0, uniqueParagraphs: 0,
        businessId: config.businessId,
        source: sourceForEarlyExit,
        embeddingsStatus: 'No PDF content to process'
    };
  }
  
  // Step 2: If initial processing was successful, proceed to LLM-based content processing.
  return processPdfContent(config, crawlOutput);
}
